{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "\n",
    "<img src=\"./slides/Slide1.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide11.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide12.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Classical NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked word prediction with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/llm_agents_lecture/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldest canadian professional national dominant "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"The Toronto Blue Jays are the [MASK] team in baseball.\"\n",
    "input_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input_tensor == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(input_tensor).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked word prediction with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best  hottest  worst  greatest  top "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "sentence = \"The Toronto Blue Jays are the <mask> team in baseball.\"\n",
    "input_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input_tensor == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(input_tensor).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflection Point - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide20.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide21.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitive Landscape for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide27.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide28.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Concepts and Abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide30.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Sentiment Prediction with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just had the best day ever with my friends!\n",
      "Sentiment: Just\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I'm so disappointed with the service at the restaurant.\n",
      "Sentiment: Tweet\n",
      "\n",
      "Tweet: Looks like it's going to rain all week. Oh well, more time for coding!\n",
      "Sentiment: Fals\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Can't believe I got the job! Dreams do come true!\n",
      "Sentiment: True\n",
      "\n",
      "Tweet: Not feeling well today, think I caught a cold.\n",
      "Sentiment: Fals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "def analyze_tweet_sentiment(tweet):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "    # Task prefix\n",
    "    prompt = f\"Tweet: {tweet} Sentiment: \"\n",
    "\n",
    "    # Encode the prompt and convert to Tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate sentiment\n",
    "    sentiment_ids = model.generate(input_ids, max_length=3, num_return_sequences=1)\n",
    "    sentiment = tokenizer.decode(sentiment_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Predefined tweets\n",
    "tweets = [\n",
    "    \"Just had the best day ever with my friends!\",\n",
    "    \"I'm so disappointed with the service at the restaurant.\",\n",
    "    \"Looks like it's going to rain all week. Oh well, more time for coding!\",\n",
    "    \"Can't believe I got the job! Dreams do come true!\",\n",
    "    \"Not feeling well today, think I caught a cold.\",\n",
    "]\n",
    "\n",
    "# Analyze sentiment of each tweet\n",
    "for tweet in tweets:\n",
    "    sentiment = analyze_tweet_sentiment(tweet)\n",
    "    print(f\"Tweet: {tweet}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Prediction w/ HuggingFace Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Just had the best day ever with my friends!\n",
      "Sentiment: POSITIVE, Confidence: 0.9998741149902344\n",
      "\n",
      "Text: I'm so disappointed with the service at the restaurant.\n",
      "Sentiment: NEGATIVE, Confidence: 0.999789297580719\n",
      "\n",
      "Text: Looks like it's going to rain all week. Oh well, more time for coding!\n",
      "Sentiment: NEGATIVE, Confidence: 0.9966244697570801\n",
      "\n",
      "Text: Can't believe I got the job! Dreams do come true!\n",
      "Sentiment: POSITIVE, Confidence: 0.9995515942573547\n",
      "\n",
      "Text: Not feeling well today, think I caught a cold.\n",
      "Sentiment: NEGATIVE, Confidence: 0.999713122844696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "tweets = [\n",
    "    \"Just had the best day ever with my friends!\",\n",
    "    \"I'm so disappointed with the service at the restaurant.\",\n",
    "    \"Looks like it's going to rain all week. Oh well, more time for coding!\",\n",
    "    \"Can't believe I got the job! Dreams do come true!\",\n",
    "    \"Not feeling well today, think I caught a cold.\",\n",
    "]\n",
    "\n",
    "# Analyze sentiment\n",
    "for text in tweets:\n",
    "    result = sentiment_pipeline(text)\n",
    "    print(\n",
    "        f\"Text: {text}\\nSentiment: {result[0]['label']}, Confidence: {result[0]['score']}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Sentiment Analysis w/ Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just had the best day ever with my friends!\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: I'm so disappointed with the service at the restaurant.\n",
      "Sentiment: The sentiment of this tweet is: **Negative**\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Looks like it's going to rain all week. Oh well, more time for coding!\n",
      "Sentiment: I would categorize the sentiment of this tweet as:\n",
      "\n",
      "**Positive**\n",
      "\n",
      "The tone is lighthearted and optimistic, with a focus on finding a silver lining in the rain (more time to code!). The overall sentiment is upbeat and enthusiastic.\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Can't believe I got the job! Dreams do come true!\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Not feeling well today, think I caught a cold.\n",
      "Sentiment: The sentiment of the tweet is:\n",
      "\n",
      "**Negative**\n",
      "\n",
      "The tweet expresses a negative emotion as the person mentions not feeling well and thinks they have caught a cold.\n",
      "\n",
      "\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "tweets = [\n",
    "    \"Just had the best day ever with my friends!\",\n",
    "    \"I'm so disappointed with the service at the restaurant.\",\n",
    "    \"Looks like it's going to rain all week. Oh well, more time for coding!\",\n",
    "    \"Can't believe I got the job! Dreams do come true!\",\n",
    "    \"Not feeling well today, think I caught a cold.\",\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    response = llm.invoke(\n",
    "        f\"Analyze the sentiment of the tweet: {tweet}\\nRespond precisely with one of ['positive', 'negative', 'neutral']\"\n",
    "    )\n",
    "    print(f\"Sentiment: {response}\\n\")\n",
    "    print(\"\\n***\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Sentiment Analysis w/ Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just had the best day ever with my friends!\n",
      "Sentiment: positive\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: I'm so disappointed with the service at the restaurant.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Looks like it's going to rain all week. Hooray! More time for work!\n",
      "Sentiment: positive (with a hint of sarcasm)\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Can't believe I got the job! Dreams do come true!\n",
      "Sentiment: positive\n",
      "\n",
      "\n",
      "***\n",
      "\n",
      "Tweet: Not feeling well today, think I caught a cold.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = \"\"\"1. Tweet: \"Wow, just watched an incredible sunset from my balcony!\"\n",
    "   Sentiment: positive\n",
    "\n",
    "2. Tweet: \"Frustrated with how slow my internet has been today.\"\n",
    "   Sentiment: negative\n",
    "\n",
    "3. Tweet: \"Nothing special happening today, just a typical Monday.\"\n",
    "   Sentiment: neutral\n",
    "\n",
    "4. Tweet: \"Just got back from an amazing vacation in Hawaii!\"\n",
    "   Sentiment: positive\n",
    "\n",
    "5. Tweet: \"Feeling under the weather after yesterday's marathon.\"\n",
    "   Sentiment: negative\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "# Role\n",
    "You are an expert sentiment analysis agent trained to analyze tweets. \n",
    "Your task is to determine whether the sentiment expressed in a tweet is positive, negative, or neutral.\n",
    "\n",
    "# Examples\n",
    "The following examples illustrate the expected response format:\n",
    "Tweet: \"Wow, just watched an incredible sunset from my balcony!\"\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: \"Frustrated with how slow my internet has been today.\"\n",
    "Sentiment: negative\n",
    "\n",
    "Tweet: \"Nothing special happening today, just a typical Monday.\"\n",
    "Sentiment: neutral\n",
    "\n",
    "Tweet: \"Just got back from an amazing vacation in Hawaii!\"\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: \"Feeling under the weather after yesterday's marathon.\"\n",
    "Sentiment: negative\n",
    "\n",
    "# Instructions\n",
    "Analyze the sentiment of the following tweet and respond with exactly one of the options: 'positive', 'negative', 'neutral'. \n",
    "If you detect sarcasm, please assign a negative score.\n",
    "\n",
    "# Your Task\n",
    "Tweet: \"{tweet}\"\n",
    "\"\"\"\n",
    "\n",
    "tweets = [\n",
    "    \"Just had the best day ever with my friends!\",\n",
    "    \"I'm so disappointed with the service at the restaurant.\",\n",
    "    \"Looks like it's going to rain all week. Hooray! More time for work!\",\n",
    "    \"Can't believe I got the job! Dreams do come true!\",\n",
    "    \"Not feeling well today, think I caught a cold.\",\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    response = llm.invoke(\n",
    "        prompt_template.format(tweet=tweet)\n",
    "    )\n",
    "    print(f\"{response}\\n\")\n",
    "    print(\"\\n***\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide31.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide32.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide33.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Thoughts Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4-turbo-preview may change over time. Returning num tokens assuming gpt-4-0125-preview.\n"
     ]
    }
   ],
   "source": [
    "# Set up Council to help with OpenAI LLM calls\n",
    "\n",
    "from council.contexts import AgentContext, Budget\n",
    "from council.llm import OpenAILLM, LLMMessage\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "llm_gpt = OpenAILLM.from_env()\n",
    "\n",
    "def invoke_GPT(prompt, llm=llm_gpt, system_prompt=None, context=None):\n",
    "    if context is None:\n",
    "        context = AgentContext.empty(budget=Budget(200))\n",
    "    if system_prompt:\n",
    "        messages = [\n",
    "            LLMMessage.system_message(system_prompt),\n",
    "            LLMMessage.user_message(prompt),\n",
    "        ]\n",
    "    else:\n",
    "        messages = [LLMMessage.user_message(prompt)]\n",
    "    response = llm.post_chat_request(context=context, messages=messages)\n",
    "    return response.first_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message to GPT:\n",
      "User Question: Can you please help me create a sales forecasting model that incorporates social media analysis?\n",
      "\n",
      "Business Analyst: To start, we need to define the scope of the sales forecasting model. This includes identifying the key performance indicators (KPIs) that are most relevant to our sales objectives and understanding how social media metrics might influence these KPIs. We should also consider the time frame for our forecast—whether it's short-term (monthly, quarterly) or long-term (annual).\n",
      "\n",
      "Data Scientist: Agreed on the importance of defining scope and KPIs. From a data science perspective, we need to identify the types of social media data that could be relevant to our sales forecasting model. This might include social media sentiment analysis, trends in hashtag usage related to our products or industry, and the volume of social media mentions. We'll also need to discuss the methodologies for integrating this social media data with traditional sales data to enhance the forecasting model.\n",
      "\n",
      "Executive Sponsor: It's crucial that the model aligns with our overall business strategy and objectives. I'm particularly interested in how the integration of social media analysis can give us a competitive edge and improve our responsiveness to market trends. Let's ensure we have a clear understanding of the resources required for this project, including data sources, technology, and expertise, and how it fits into our strategic priorities.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response: agree with the above, what methodologies would you suggest?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Analyst: Given the user's interest in methodologies, I suggest we start by mapping out the customer journey to understand where social media impacts the buying process. This will help us identify specific social media metrics that correlate with sales outcomes. For integrating social media data with sales data, methodologies such as regression analysis to identify correlations, and machine learning models for predictive analytics could be effective. We should also consider the use of A/B testing to refine our understanding of how social media influences sales.\n",
      "\n",
      "Data Scientist: To build on the Business Analyst's suggestions, for the data integration and modeling part, I recommend using time series analysis to forecast sales trends, incorporating social media metrics as external variables. Machine learning models, such as Random Forest or Gradient Boosting Machines, can be particularly useful for handling the complex relationships between social media activities and sales outcomes. Natural Language Processing (NLP) techniques will be essential for sentiment analysis and understanding the context of social media mentions. We should also explore the use of advanced analytics techniques like causal inference models to better understand the impact of specific social media campaigns on sales.\n",
      "\n",
      "Executive Sponsor: I support the proposed methodologies. It's important that whatever methodologies we choose, they must be scalable and adaptable to changing market conditions and social media trends. Additionally, ensuring that our approach is data-driven and evidence-based will be key to securing buy-in from stakeholders. Let's also consider the potential for real-time analytics to adjust our sales strategies dynamically in response to social media trends. This could be a significant competitive advantage.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response: stop\n"
     ]
    }
   ],
   "source": [
    "tot_system_prompt = \"\"\"\n",
    "Imagine three different experts are having a meeting to discuss an important User Question.\n",
    "\n",
    "The experts are:\n",
    "- Business Analyst\n",
    "- Data Scientist\n",
    "- Executive Sponsor\n",
    "\n",
    "All experts will write down one step of their thinking at-a-time, then share it with the group.\n",
    "If an expert doesn't have anything to add to the current part of the discussion, they will just say \"pass\".\n",
    "\n",
    "Whenever an expert is speaking, their messages should be labelled with their title.\n",
    "We will conduct this meeting one step at a time.\n",
    "At each step, each expert must provide their response.\n",
    "After each round, you will receive User Input. Use this to steer the next steps of the meeting.\n",
    "\n",
    "Please go ahead with the meeting.\n",
    "\"\"\"\n",
    "\n",
    "task = \"User Question: Can you please help me create a sales forecasting model that incorporates social media analysis?\"\n",
    "\n",
    "message = task\n",
    "print(f\"Sending message to GPT:\\n{message}\\n\")\n",
    "\n",
    "messages = [message]  # Collect messages\n",
    "while True:\n",
    "\n",
    "     # Send messages to LLM\n",
    "    llm_response = invoke_GPT(\"\\n\\n\".join(messages), system_prompt=tot_system_prompt) \n",
    "\n",
    "    # Record the LLM response\n",
    "    messages.append(llm_response)\n",
    "\n",
    "    # Display the messages\n",
    "    print(f\"{llm_response}\\n\")\n",
    "\n",
    "    # Collect User Input\n",
    "    message = input(\"Enter your response:\")\n",
    "    messages.append(f\"User Input: {message}\\n\")\n",
    "    if message == \"stop\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide34.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message to GPT:\n",
      "How can forecast food prices?\n",
      "\n",
      "### Thought\n",
      "To forecast food prices, we need to understand the factors that influence these prices. These can include current market trends, historical price data, supply and demand dynamics, weather conditions, geopolitical events, and economic indicators. Forecasting models often use statistical or machine learning techniques to predict future prices based on these factors.\n",
      "\n",
      "### Action\n",
      "To get a foundational understanding of the methods used to forecast food prices, I will perform a web search.\n",
      "\n",
      "### Execute\n",
      "web_search(\"how to forecast food prices\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response: use the FRED database and linear regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Observation\n",
      "The web search suggests using the FRED (Federal Reserve Economic Data) database for accessing historical price data and employing linear regression techniques for forecasting food prices. This approach involves statistical analysis where past price data is used to predict future prices, considering the linear relationship between time and price changes.\n",
      "\n",
      "### Thought\n",
      "Given the suggestion, the next step is to identify specific food items or categories for which we want to forecast prices. We should then access the FRED database to retrieve historical price data for these items. After obtaining the data, we can apply linear regression analysis to make the forecasts.\n",
      "\n",
      "### Action\n",
      "To proceed effectively, I need to decide on the food items or categories to focus on. I will ask the user to specify which food items or categories they are interested in forecasting.\n",
      "\n",
      "### Execute\n",
      "ask_question(\"Which specific food items or categories are you interested in forecasting prices for?\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response from GPT:\n",
      "### Thought\n",
      "With the availability of customer sentiment data in JSON format and historical sales data in Excel spreadsheets, the next steps involve processing these datasets to a usable format for analysis and model building. The sentiment data will likely require cleaning and sentiment analysis to convert textual data into numerical scores. The sales data will need to be aggregated or transformed to match the sentiment data's granularity, ensuring that the analysis correlates sentiment to sales effectively. Given the formats, we'll need to convert the JSON and Excel files into a format that can be easily manipulated and analyzed, such as a pandas DataFrame in Python.\n",
      "\n",
      "### Action\n",
      "Before proceeding with data processing, it's essential to convert the JSON sentiment data and Excel sales data into pandas DataFrames. This will facilitate easier manipulation and analysis in the subsequent steps. Since I can't directly convert files here, I'll simulate querying a database to represent this conversion process for both datasets.\n",
      "\n",
      "**Action:** query_database(\"SELECT * FROM sentiment_data\")\n",
      "\n",
      "### Observation\n",
      "Please wait for the Observation to proceed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your response: stop\n"
     ]
    }
   ],
   "source": [
    "react_system_prompt = \"\"\"Solve a problem with interleaving steps:\n",
    "1. Thought\n",
    "2. Action\n",
    "3. Observation\n",
    "\n",
    "where each Action must be a use of one of the following functions: \n",
    "[\n",
    "    ask_question(question:str) -> str,  # Send a question to the user and receive a response\n",
    "    web_search(search_query:str) -> str,  # Perform a web search and return summaries of the top 5 results\n",
    "    calculator(op:ArithOperation, a:float, b:float) -> float,  # Perform a simple arithmetic operation\n",
    "    query_database(query:str) -> pd.DataFrame,  # Query a database and return the results as a DataFrame\n",
    "    analyze_data(data:pd.DataFrame) -> str  # Analyze a DataFrame and return a summary\n",
    "]\n",
    "\n",
    "Let's tackle the problem one step at a time.\n",
    "Whenever you invoke an Action, please wait for me to provide the next Observation before you proceed.\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"How can forecast food prices?\"\"\"\n",
    "\n",
    "message = task\n",
    "print(f\"Sending message to GPT:\\n{message}\\n\")\n",
    "\n",
    "messages = [message]  # Collect messages\n",
    "while True:\n",
    "\n",
    "     # Send messages to LLM\n",
    "    llm_response = invoke_GPT(\"\\n\\n\".join(messages), system_prompt=react_system_prompt) \n",
    "\n",
    "    # Record the LLM response\n",
    "    messages.append(llm_response)\n",
    "\n",
    "    # Display the messages\n",
    "    print(f\"{llm_response}\\n\")\n",
    "\n",
    "    # Collect User Input\n",
    "    message = input(\"Enter your response:\")\n",
    "    messages.append(f\"User Input: {message}\\n\")\n",
    "    if message == \"stop\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"GPTs\" Data Analyst\n",
    "\n",
    "(In Browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an AI Agent with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide35.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy: \"Programming—not prompting—Foundation Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./slides/Slide37.png\" alt=\"Image description\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "from typing import List\n",
    "import wget\n",
    "\n",
    "import dspy\n",
    "from dspy import Signature, InputField, OutputField\n",
    "from dspy.functional import TypedPredictor\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "dspy.settings.configure(lm=dspy.OpenAI(model=\"gpt-4o\", max_tokens=2048))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIImage(BaseModel):\n",
    "    \"\"\"A single generated image.\"\"\"\n",
    "\n",
    "    prompt: str = Field(desc=\"The prompt used to generate the image.\")\n",
    "    url: str = Field(\n",
    "        desc=\"The URL of the generated image.\", default=\"./img/placeholder.webp\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Slide(BaseModel):\n",
    "    \"\"\"A single slide in a lecture.\"\"\"\n",
    "\n",
    "    title: str = Field(desc=\"The slide's title.\")\n",
    "    bullets: List[str] = Field(\n",
    "        desc=\"Up to 5 bullet points of concise, relevant content.\"\n",
    "    )\n",
    "    image: AIImage = Field(desc=\"A nice AI generated image to accompany the slide.\")\n",
    "    python_code_example: str = Field(\n",
    "        desc=\"An optional Python code example to include in the slide.\", default=None\n",
    "    )\n",
    "\n",
    "    def to_html(self):\n",
    "        html_output = f'<h2>{self.title}</h2><table><tr><td><img src=\"{self.image.url}\" width=\"400\" alt=\"{self.image.prompt}\"></td><td>'\n",
    "        for bullet in self.bullets:\n",
    "            html_output += f\"<li>{bullet}</li>\"\n",
    "        if self.python_code_example:\n",
    "            html_output += f\"<pre><code>{self.python_code_example}</code></pre>\"\n",
    "        html_output += \"</td></tr></table><hr>\"\n",
    "        return html_output\n",
    "\n",
    "\n",
    "class Lecture(BaseModel):\n",
    "    \"\"\"A complete lecture with a title, description, and content.\"\"\"\n",
    "\n",
    "    title: str = Field(desc=\"The lecture's title.\")\n",
    "    description: str = Field(desc=\"A brief description of the lecture.\")\n",
    "    slides: List[Slide] = Field(desc=\"The slides that make up the lecture.\")\n",
    "\n",
    "    def to_html(self):\n",
    "        html_output = f\"<h1>{self.title}</h1><p>{self.description}</p><hr>\"\n",
    "        for slide in self.slides:\n",
    "            html_output += slide.to_html()\n",
    "        return html_output\n",
    "\n",
    "\n",
    "class LectureCreator(Signature):\n",
    "    \"\"\"Create content for a great lecture.\"\"\"\n",
    "\n",
    "    lecture_subject: str = InputField(desc=\"The subject of the lecture.\")\n",
    "    lecture_content: Lecture = OutputField(desc=\"The complete lecture content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_my_lecture(subject:str):\n",
    "    lecture_creator = TypedPredictor(LectureCreator)\n",
    "    lecture = lecture_creator(lecture_subject=subject)\n",
    "    for slide in lecture.lecture_content.slides:\n",
    "        prompt = slide.image.prompt\n",
    "        print(f\"Calling OpenAI to generate an image for the prompt: {prompt}\")\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                # Call OpenAI to generate the image\n",
    "                dalle_response = client.images.generate(\n",
    "                    model=\"dall-e-3\",\n",
    "                    prompt=prompt,\n",
    "                    size=\"1024x1024\",\n",
    "                    quality=\"standard\",\n",
    "                    n=1,\n",
    "                )\n",
    "                # Download and save it\n",
    "                image_url = dalle_response.data[0].url\n",
    "                image_filename = wget.download(image_url, out=\"./img\")\n",
    "                slide.image.url = image_filename\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error calling OpenAI: {e}, retrying after 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "    # Save the markdown to a file\n",
    "    with open(f\"{subject}_lecture.html\", \"w\") as file:\n",
    "        file.write(lecture.lecture_content.to_html())\n",
    "\n",
    "    return lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling OpenAI to generate an image for the prompt: A diverse group of people analyzing social media posts on a large screen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 3162696 / 3162696Calling OpenAI to generate an image for the prompt: A graph showing sales trends influenced by social media sentiment\n",
      "100% [......................................................] 3162696 / 3162696Calling OpenAI to generate an image for the prompt: A computer screen displaying code and sentiment analysis results\n",
      "100% [......................................................] 3162696 / 3162696Calling OpenAI to generate an image for the prompt: A Python code editor with sentiment analysis code\n",
      "100% [......................................................] 3162696 / 3162696Calling OpenAI to generate an image for the prompt: A team of data scientists working on sales forecasting using sentiment analysis\n",
      "100% [......................................................] 3162696 / 3162696Calling OpenAI to generate an image for the prompt: A futuristic representation of AI and sentiment analysis\n",
      "100% [......................................................] 3162696 / 3162696"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>Sentiment Analysis for Sales Prediction</h1><p>This lecture explores the application of sentiment analysis in predicting sales trends. We will cover the basics of sentiment analysis, its importance in sales forecasting, and practical implementation using Python.</p><hr><h2>Introduction to Sentiment Analysis</h2><table><tr><td><img src=\"./img/img-4kESvTqenL5JDsoFgramYanb.png\" width=\"400\" alt=\"A diverse group of people analyzing social media posts on a large screen\"></td><td><li>Definition of Sentiment Analysis</li><li>Importance in various industries</li><li>Basic techniques and tools</li></td></tr></table><hr><h2>Sentiment Analysis in Sales Prediction</h2><table><tr><td><img src=\"./img/img-1Jlj3lZ0sUIJgqpKAcObu7Wu.png\" width=\"400\" alt=\"A graph showing sales trends influenced by social media sentiment\"></td><td><li>How sentiment affects consumer behavior</li><li>Case studies of sentiment analysis in sales</li><li>Benefits of using sentiment analysis for sales forecasting</li></td></tr></table><hr><h2>Tools and Techniques</h2><table><tr><td><img src=\"./img/img-kAhgyz5lYf8z0EKoMazg8S9n.png\" width=\"400\" alt=\"A computer screen displaying code and sentiment analysis results\"></td><td><li>Overview of popular sentiment analysis tools</li><li>Introduction to Natural Language Processing (NLP)</li><li>Machine learning models for sentiment analysis</li></td></tr></table><hr><h2>Implementing Sentiment Analysis in Python</h2><table><tr><td><img src=\"./img/img-VVMWn8fJ5enQrQlhUgVgOYpZ.png\" width=\"400\" alt=\"A Python code editor with sentiment analysis code\"></td><td><li>Setting up the environment</li><li>Using libraries like NLTK and TextBlob</li><li>Building a simple sentiment analysis model</li><pre><code>import nltk\n",
       "from textblob import TextBlob\n",
       "\n",
       "# Sample text\n",
       "text = 'I love this product! It has changed my life.'\n",
       "\n",
       "# Create a TextBlob object\n",
       "blob = TextBlob(text)\n",
       "\n",
       "# Get the sentiment\n",
       "sentiment = blob.sentiment\n",
       "print(sentiment)</code></pre></td></tr></table><hr><h2>Case Study: Sentiment Analysis for Sales Forecasting</h2><table><tr><td><img src=\"./img/img-XXC9uLqaE5vdZgFP0M01XPHo.png\" width=\"400\" alt=\"A team of data scientists working on sales forecasting using sentiment analysis\"></td><td><li>Overview of the case study</li><li>Data collection and preprocessing</li><li>Model training and evaluation</li></td></tr></table><hr><h2>Challenges and Future Directions</h2><table><tr><td><img src=\"./img/img-r3QFD6rhlXsnqodO9Oh1tSQz.png\" width=\"400\" alt=\"A futuristic representation of AI and sentiment analysis\"></td><td><li>Common challenges in sentiment analysis</li><li>Ethical considerations</li><li>Future trends and advancements</li></td></tr></table><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lecture = create_my_lecture(\"Sentiment Analysis for Sales Prediction\")\n",
    "display(HTML(lecture.lecture_content.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_schema = \"\"\"kind: DatasetMetadata\n",
    "version: 0.1\n",
    "metadata:\n",
    "  name: AirBNB\n",
    "spec:\n",
    "  desc: \"New York City Airbnb Open Data\"\n",
    "  tables:\n",
    "    - name: AB_NYC_2019\n",
    "      desc: 'Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities\n",
    "    and present more unique, personalized way of experiencing the world. This dataset\n",
    "    describes the listing activity and metrics in NYC, NY for 2019.\n",
    "\n",
    "    Content\n",
    "\n",
    "    This data file includes all needed information to find out more about hosts, geographical\n",
    "    availability, necessary metrics to make predictions and draw conclusions.'\n",
    "      columns:\n",
    "      - desc: listing ID\n",
    "        name: id\n",
    "      - desc: name of the listing\n",
    "        name: name\n",
    "      - desc: host ID\n",
    "        name: host_id\n",
    "      - desc: name of the host\n",
    "        name: host_name\n",
    "      - desc: location\n",
    "        name: neighbourhood_group\n",
    "      - desc: area\n",
    "        name: neighbourhood\n",
    "      - desc: latitude coordinates\n",
    "        name: latitude\n",
    "      - desc: longitude coordinates\n",
    "        name: longitude\n",
    "      - desc: listing space type\n",
    "        name: room_type\n",
    "      - desc: price in dollars\n",
    "        name: price\n",
    "      - desc: amount of nights minimum\n",
    "        name: minimum_nights\n",
    "      - desc: number of reviews\n",
    "        name: number_of_reviews\n",
    "      - desc: latest review\n",
    "        name: last_review\n",
    "      - desc: number of review per month\n",
    "        name: reviews_per_month\n",
    "      - desc: amount of listing per host\n",
    "        name: calculated_host_listings_count\n",
    "      - desc: number of days when listing is available for booking\n",
    "        name: availability_365\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating query...\n",
      "\n",
      "Done!\n",
      "\n",
      "Executing query:\n",
      "\n",
      "WITH percentiles AS (\n",
      "    SELECT\n",
      "        neighbourhood_group,\n",
      "        percentile_cont(0.25) WITHIN GROUP (ORDER BY price) AS p25,\n",
      "        percentile_cont(0.50) WITHIN GROUP (ORDER BY price) AS p50,\n",
      "        percentile_cont(0.75) WITHIN GROUP (ORDER BY price) AS p75\n",
      "    FROM\n",
      "        AB_NYC_2019\n",
      "    GROUP BY\n",
      "        neighbourhood_group\n",
      ")\n",
      "SELECT\n",
      "    neighbourhood_group,\n",
      "    p25 AS \"25th Percentile\",\n",
      "    p50 AS \"50th Percentile (Median)\",\n",
      "    p75 AS \"75th Percentile\"\n",
      "FROM\n",
      "    percentiles\n",
      "ORDER BY\n",
      "    neighbourhood_group;\n",
      "\n",
      "  neighbourhood_group  25th Percentile  50th Percentile (Median)  \\\n",
      "0               Bronx             45.0                      65.0   \n",
      "1            Brooklyn             60.0                      90.0   \n",
      "2           Manhattan             95.0                     150.0   \n",
      "3              Queens             50.0                      75.0   \n",
      "4       Staten Island             50.0                      75.0   \n",
      "\n",
      "   75th Percentile  \n",
      "0             99.0  \n",
      "1            150.0  \n",
      "2            220.0  \n",
      "3            110.0  \n",
      "4            110.0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from sqlalchemy import create_engine\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Assuming dspy is properly imported and configured\n",
    "import dspy\n",
    "from dspy.functional import TypedPredictor\n",
    "from dspy import Signature, InputField, OutputField\n",
    "\n",
    "# Define the database URI and schema\n",
    "database_URI = \"postgresql+psycopg2://postgres:postgres@localhost:5432/nyc_airbnb\"\n",
    "\n",
    "def parse_code_block(code_block, kind):\n",
    "    pattern = f\"```{kind}(.*?)```\"\n",
    "    match = re.search(pattern, code_block, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "class SQLCodeGenerator(Signature):\n",
    "    \"\"\"Generate PostgreSQL code to access data from the database.\"\"\"\n",
    "    task: str = InputField(desc=\"The Database Specialist's task in natural language.\")\n",
    "    database_schema: str = InputField(desc=\"The database's schema.\")\n",
    "    database_URI: str = InputField(desc=\"The database's URI.\")\n",
    "    thoughts: str = OutputField(desc=\"The Database Specialist's high-level plan for writing a great query.\")\n",
    "    sql_code_block: str = OutputField(desc=\"The SQL code block to access the data.\")\n",
    "\n",
    "class DatabaseSpecialist(BaseModel):\n",
    "    \"\"\"Write and execute a PostgreSQL query to access data from the database.\"\"\"\n",
    "    database_task: str = Field(desc=\"A natural-language task for the DatabaseSpecialist.\")\n",
    "    database_schema: str = Field(desc=\"The database schema.\")\n",
    "\n",
    "    def execute_query(self, query) -> pd.DataFrame:\n",
    "        print(f\"Executing query:\\n\\n{query}\\n\")\n",
    "        try:\n",
    "            engine = create_engine(database_URI)\n",
    "            with engine.connect() as connection:\n",
    "                df = pd.read_sql_query(query, connection)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Query execution failed: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "\n",
    "    def execute(self) -> pd.DataFrame:\n",
    "        print(\"Generating query...\\n\")\n",
    "        code_generator = TypedPredictor(SQLCodeGenerator)\n",
    "        response = code_generator(task=self.database_task, database_schema=self.database_schema, database_URI=database_URI)\n",
    "        print(\"Done!\\n\")\n",
    "        sql_query = parse_code_block(response.sql_code_block, kind=\"sql\")\n",
    "        return self.execute_query(sql_query)\n",
    "\n",
    "# Usage example\n",
    "result = DatabaseSpecialist(\n",
    "    # database_task=\"Please get me the average price and standard deviation by borough.\",\n",
    "    database_task=\"Can you please show me a percentile breakdown of prices by borough?\",\n",
    "    database_schema=airbnb_schema\n",
    ").execute()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
